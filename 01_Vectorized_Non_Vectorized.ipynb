{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a0f7a",
   "metadata": {},
   "source": [
    "# Vectoried and Non Vectorized Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3ae54",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the substantial performance difference between vectorized and non-vectorized implementations for a simple neural network's forward pass. By comparing the execution times, we highlight the efficiency gains achieved by leveraging NumPy's optimized matrix operations. This is a fundamental concept in deep learning for processing large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af157c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6aa58",
   "metadata": {},
   "source": [
    "## Sigmoid Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7ed87",
   "metadata": {},
   "source": [
    "#### We define the sigmoid activation function, which is a common choice in binary classification problems. It squashes the input values to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da4b7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid activation\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7bc0a",
   "metadata": {},
   "source": [
    "## Neural Networks and Data Parameters\n",
    "\n",
    "#### Next, we define the architecture of our neural network and the dimensions of our dataset:\n",
    "\n",
    "* m: The number of training examples (1000).\n",
    "\n",
    "* n_x: The number of input features (10).\n",
    "\n",
    "* n_h: The number of units in the hidden layer (5).\n",
    "\n",
    "* n_y: The number of output units (1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6befa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "np.random.seed(1)\n",
    "m = 1000  # Number of examples\n",
    "n_x = 10  # Input features\n",
    "n_h = 5   # Hidden units\n",
    "n_y = 1   # Output unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a003011",
   "metadata": {},
   "source": [
    "## Data and Weights Initialization\n",
    "\n",
    "* Input Data (X): We create a random dataset X with dimensions (n_x, m), where each column represents a training example.\n",
    "* Weights and Biases (W1, b1, W2, b2): We initialize the weight matrices and bias vectors for the single hidden layer and the output layer. The weights are initialized with small random values to break symmetry, and biases are initialized to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2d43e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.random.randn(n_x, m)\n",
    "# Parameters\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "b2 = np.zeros((n_y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c56b8b",
   "metadata": {},
   "source": [
    "## Non-Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5010eb2",
   "metadata": {},
   "source": [
    "#### In this approach, we use a for loop to iterate through each of the m training examples one by one. For each example x_i, we compute the hidden layer activation a1_i and the final output a2_i. The results for each example are stored in the A2_nonvec matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "924aa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nonvec = time.time()\n",
    "\n",
    "A2_nonvec = np.zeros((n_y, m))  # Here the final predictions are stored\n",
    "\n",
    "for i in range(m):\n",
    "    x_i = X[:, i].reshape(n_x, 1)              # (n_x, 1)\n",
    "    \n",
    "    z1_i = np.dot(W1, x_i) + b1                # (n_h, 1)\n",
    "    a1_i = sigmoid(z1_i)                       # (n_h, 1)\n",
    "    \n",
    "    z2_i = np.dot(W2, a1_i) + b2               # (n_y, 1)\n",
    "    a2_i = sigmoid(z2_i)                       # (n_y, 1)\n",
    "    \n",
    "    A2_nonvec[:, i] = a2_i.flatten()\n",
    "end_nonvec = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d809f",
   "metadata": {},
   "source": [
    "## Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62693",
   "metadata": {},
   "source": [
    "#### Here, we eliminate the explicit for loop. Instead of processing one example at a time, we feed the entire dataset X (a (n_x, m) matrix) into the network at once. NumPy's optimized functions handle the matrix multiplications and additions efficiently across all m examples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42fc0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vec = time.time()\n",
    "\n",
    "Z1 = np.dot(W1, X) + b1                        # (n_h, m)\n",
    "A1 = sigmoid(Z1)                               # (n_h, m)\n",
    "\n",
    "Z2 = np.dot(W2, A1) + b2                       # (n_y, m)\n",
    "A2 = sigmoid(Z2)                               # (n_y, m)\n",
    "\n",
    "end_vec = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54486bee",
   "metadata": {},
   "source": [
    "### Performance Time\n",
    "#### We measure and print the time taken by both implementations. The output clearly shows that the vectorized implementation is significantly faster than the non-vectorized one.\n",
    "* Non-Vectorized Time: 0.019034 seconds\n",
    "* Vectorized Time: 0.001000 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83abeeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Vectorized Time: 0.013117 seconds\n",
      "Vectorized Time    : 0.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Non-Vectorized Time: {end_nonvec - start_nonvec:.6f} seconds\")\n",
    "print(f\"Vectorized Time    : {end_vec - start_vec:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feace3e",
   "metadata": {},
   "source": [
    "### Correctness Check\n",
    "#### To ensure that both methods produce the same result, we calculate the Frobenius norm of the difference between the output matrices A2 (from the vectorized approach) and A2_nonvec (from the non-vectorized approach).\n",
    "* Difference = 0.0000000000\n",
    "#### The difference is zero, confirming that both implementations are mathematically equivalent and yield the exact same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26160057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between vectorized and non-vectorized outputs: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "diff = np.linalg.norm(A2 - A2_nonvec)\n",
    "print(f\"Difference between vectorized and non-vectorized outputs: {diff:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b31e95",
   "metadata": {},
   "source": [
    "## Prediction Check\n",
    "#### Finally, we print the first 10 predictions from both methods side-by-side. As expected from the correctness check, the predicted values for each example are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "880ed25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 predictions (Vectorized vs Non-Vectorized):\n",
      "Example  1: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  2: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  3: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  4: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  5: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  6: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  7: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example  8: Vectorized = 0.4964, Non-Vectorized = 0.4964\n",
      "Example  9: Vectorized = 0.4965, Non-Vectorized = 0.4965\n",
      "Example 10: Vectorized = 0.4964, Non-Vectorized = 0.4964\n"
     ]
    }
   ],
   "source": [
    "# Round outputs to 4 decimal places for neat display\n",
    "A2_rounded = np.round(A2, 4)\n",
    "A2_nonvec_rounded = np.round(A2_nonvec, 4)\n",
    "\n",
    "# Print first few predictions from each method\n",
    "print(\"\\nFirst 10 predictions (Vectorized vs Non-Vectorized):\")\n",
    "for i in range(10):  # You can increase this if you want\n",
    "    v = A2_rounded[0, i]\n",
    "    nv = A2_nonvec_rounded[0, i]\n",
    "    print(f\"Example {i+1:2d}: Vectorized = {v:.4f}, Non-Vectorized = {nv:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4640cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
